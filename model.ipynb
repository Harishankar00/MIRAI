{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "15ff0d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and device setup\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import timm  # For Swin Transformer model\n",
    "\n",
    "# Device setup - use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ba13508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MammographyDataset(Dataset):\n",
    "    def __init__(self, excel_path, transform=None, mask_transform=None):\n",
    "        self.df = pd.read_excel(excel_path)\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.label_map = {'Benign': 0, 'Malignant': 1, 'Normal': 2, 'Suspicious': 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Use relative paths from excel directly (no prepend)\n",
    "        image_path = Path(row['relative_image_path'])\n",
    "        mask_path = Path(row['relative_mask_path'])\n",
    "\n",
    "        # Debug print for first few\n",
    "        if idx < 3:\n",
    "            print(f\"Sample {idx} loading image from: {image_path}\")\n",
    "            print(f\"Sample {idx} loading mask from: {mask_path}\")\n",
    "\n",
    "        image = Image.open(image_path).convert('L')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        label = self.label_map[row['label']]\n",
    "        birads = row['BIRADS']\n",
    "        if pd.isna(birads):\n",
    "            birads = np.nan\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        return image, mask, label, birads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86732fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Transforms and DataLoader setup\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "\n",
    "# Image and mask transforms\n",
    "image_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.5], std=[0.5])  # Assuming grayscale normalization\n",
    "])\n",
    "\n",
    "mask_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor()  # Mask as float tensor with values 0 or 1\n",
    "])\n",
    "\n",
    "# Paths to images and masks root folders (assumed to be the parent folders of relative paths in Excel)\n",
    "image_root_dir = 'Subset/Preprocessed_Dataset'\n",
    "mask_root_dir = 'Subset/Masks'\n",
    "\n",
    "# Create dataset\n",
    "dataset = MammographyDataset(\n",
    "    excel_path='Subset/subset_catalog.xlsx',\n",
    "    transform=image_transform,\n",
    "    mask_transform=mask_transform\n",
    ")\n",
    "\n",
    "\n",
    "# Split dataset (train/val) for demonstration; adjust split as needed\n",
    "total_len = len(dataset)\n",
    "train_len = int(0.8 * total_len)\n",
    "val_len = total_len - train_len\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, val_len])\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 4  # Adjust according to GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a618cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "class MultiTaskSwin(nn.Module):\n",
    "    def __init__(self, num_classes=4, seg_out_channels=1):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, in_chans=1)\n",
    "        self.backbone.reset_classifier(0)\n",
    "\n",
    "        self.classifier = nn.Linear(self.backbone.num_features, num_classes)\n",
    "\n",
    "        self.features = None\n",
    "        def hook(module, input, output):\n",
    "            self.features = output\n",
    "\n",
    "        # Register hook on first stage to capture features with batch dim\n",
    "        self.backbone.layers[0].register_forward_hook(hook)\n",
    "\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(256, seg_out_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_logits = self.backbone(x)  # Classification logits\n",
    "        features = self.features  # Intermediate features from hook, assumed shape [B, H, W, C]\n",
    "\n",
    "        feat_map = features.permute(0, 3, 1, 2).contiguous()  # Fix channel dimension\n",
    "        seg_mask = self.segmentation_head(feat_map)\n",
    "        seg_mask = F.interpolate(seg_mask, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        return cls_logits, seg_mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb317f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss functions, optimizer, and scheduler set up.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Loss functions and optimizer setup\n",
    "\n",
    "# Losses\n",
    "classification_criterion = nn.CrossEntropyLoss()\n",
    "segmentation_criterion = nn.BCELoss()  # Since segmentation output uses sigmoid and mask is binary\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "model = MultiTaskSwin(num_classes=4, seg_out_channels=1).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "print(\"Loss functions, optimizer, and scheduler set up.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Training and Validation Loop with Early Stopping and Checkpointing\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion_cls, criterion_seg, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, masks, labels, birads in tqdm(train_loader, desc=\"Train Epoch\"):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cls_logits, seg_mask = model(images)\n",
    "        \n",
    "        # Classification loss\n",
    "        loss_cls = criterion_cls(cls_logits, labels)\n",
    "\n",
    "        # Segmentation loss\n",
    "        loss_seg = criterion_seg(seg_mask, masks)\n",
    "\n",
    "        # Combined loss with weights (you can tune weights)\n",
    "        loss = loss_cls + 0.5 * loss_seg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        predicted = cls_logits.argmax(dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion_cls, criterion_seg, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks, labels, birads in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            cls_logits, seg_mask = model(images)\n",
    "\n",
    "            loss_cls = criterion_cls(cls_logits, labels)\n",
    "            loss_seg = criterion_seg(seg_mask.squeeze(1), masks)\n",
    "            loss = loss_cls + 0.5 * loss_seg\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            predicted = cls_logits.argmax(dim=1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ace3dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training Controller with Early Stopping, Checkpointing, and Plots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion_cls, criterion_seg,\n",
    "                optimizer, scheduler, device, num_epochs=20, patience=5,\n",
    "                checkpoint_dir='./checkpoints'):\n",
    "\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    best_val_acc = 0\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion_cls, criterion_seg, optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion_cls, criterion_seg, device)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Save checkpoint every epoch\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pth')\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        # Early Stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            early_stopping_counter = 0\n",
    "            # Save best model checkpoint separately\n",
    "            torch.save(model.state_dict(), os.path.join(checkpoint_dir, 'best_model.pth'))\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "        \n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss curve')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Val Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy curve')\n",
    "    plt.savefig(os.path.join(checkpoint_dir, 'training_history.png'))\n",
    "    plt.show()\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fbab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Inference Function for Unseen Images (JPG/JPEG/DICOM) with Preprocessing\n",
    "\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "def load_and_preprocess_image(path, transform=None):\n",
    "    \"\"\"\n",
    "    Load image from jpg/jpeg/dicom file path and apply transforms.\n",
    "    Returns tensor image ready for model input.\n",
    "    \"\"\"\n",
    "    ext = path.suffix.lower()\n",
    "    if ext in ['.jpg', '.jpeg', '.png']:\n",
    "        img = Image.open(path).convert('L')\n",
    "    elif ext == '.dcm':\n",
    "        dicom_data = pydicom.dcmread(str(path))\n",
    "        img = dicom_data.pixel_array\n",
    "        # Normalize pixel data to 0-255 for PIL loading\n",
    "        img = ((img - img.min()) / (img.max() - img.min()) * 255).astype(np.uint8)\n",
    "        img = Image.fromarray(img).convert('L')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported image format: {ext}\")\n",
    "\n",
    "    if transform:\n",
    "        img = transform(img)\n",
    "\n",
    "    # Add batch dimension\n",
    "    img = img.unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "def inference_unseen_image(model, image_path, transform, device):\n",
    "    \"\"\"\n",
    "    Run inference on unseen image and return classification and segmentation output.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img_tensor = load_and_preprocess_image(Path(image_path), transform)\n",
    "        img_tensor = img_tensor.to(device)\n",
    "\n",
    "        cls_logits, seg_mask = model(img_tensor)\n",
    "        probs = torch.softmax(cls_logits, dim=1)\n",
    "        pred_label = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "        seg_mask = seg_mask.squeeze().cpu().numpy()  # (H, W) segmentation mask\n",
    "\n",
    "    class_map = {0: 'Benign', 1: 'Malignant', 2: 'Normal', 3: 'Suspicious'}\n",
    "    return class_map[pred_label], probs.cpu().numpy(), seg_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad02956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label, probs, mask = inference_unseen_image(model, 'path_to_image.dcm', image_transform, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c26f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Visualization of Segmentation Mask Overlaid on Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_segmentation(image_path, mask, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Visualize the grayscale image with the segmentation mask overlayed.\n",
    "    Args:\n",
    "        image_path (str or Path): Path to the input image\n",
    "        mask (numpy.ndarray): Segmentation mask (H, W) normalized [0,1]\n",
    "        alpha (float): transparency for overlay\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path).convert('L')\n",
    "    img = img.resize(mask.shape[::-1])  # Resize image to match mask size\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.imshow(mask, cmap='jet', alpha=alpha)  # Overlay mask\n",
    "    plt.title('Segmentation Mask Overlay')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage after inference:\n",
    "# visualize_segmentation('path_to_image.jpg', mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f971cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:   0%|          | 0/3998 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([4, 1, 224, 224])) that is different to the input size (torch.Size([4, 1, 1, 224, 224])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[194], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      5\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./checkpoints\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassification_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion_seg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegmentation_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_dir\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[190], line 19\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion_cls, criterion_seg, optimizer, scheduler, device, num_epochs, patience, checkpoint_dir)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_seg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion_cls, criterion_seg, device)\n\u001b[1;32m     22\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[189], line 22\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, criterion_cls, criterion_seg, optimizer, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m loss_cls \u001b[38;5;241m=\u001b[39m criterion_cls(cls_logits, labels)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Segmentation loss\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m loss_seg \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion_seg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseg_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Combined loss with weights (you can tune weights)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_cls \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m loss_seg\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:706\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3521\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3519\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3521\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3522\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3523\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3524\u001b[0m     )\n\u001b[1;32m   3526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3527\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([4, 1, 224, 224])) that is different to the input size (torch.Size([4, 1, 1, 224, 224])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "# Train the multi-task model\n",
    "\n",
    "num_epochs = 20\n",
    "patience = 5\n",
    "checkpoint_dir = './checkpoints'\n",
    "\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion_cls=classification_criterion,\n",
    "    criterion_seg=segmentation_criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=patience,\n",
    "    checkpoint_dir=checkpoint_dir\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
